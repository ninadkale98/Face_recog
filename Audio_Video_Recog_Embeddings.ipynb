{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Visual Embedding based face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/easgrad/ninadnar/miniconda3/envs/bio1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio import Inference\n",
    "from scipy.spatial.distance import cdist\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../.cache/torch/pyannote/models--pyannote--embedding/snapshots/20b2db779562a3141f5eadd34a0232dbcd56d620/pytorch_model.bin`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../.cache/torch/pyannote/models--pyannote--embedding/snapshots/20b2db779562a3141f5eadd34a0232dbcd56d620/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.1+cu117. Bad things might happen unless you revert torch to 1.x.\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.1+cu117. Bad things might happen unless you revert torch to 1.x.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Total Persons :  43\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "directory = 'VIDTIMIT'  # Replace with the path to your directory\n",
    "\n",
    "labels = [name for name in os.listdir(directory) if os.path.isdir(os.path.join(directory, name))]\n",
    "\n",
    "mtcnn = MTCNN()\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "model = Model.from_pretrained(\"pyannote/embedding\", use_auth_token=\"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\")\n",
    "inference = Inference(model, window=\"whole\")\n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)\n",
    "print(\"Total Persons : \", len(labels))\n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Audio and Video Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'VIDTIMIT'\n",
    "labels = [name for name in os.listdir(directory) if os.path.isdir(os.path.join(directory, name))]\n",
    "\n",
    "embeddings_audio = {}\n",
    "embeddings_face = {}\n",
    "\n",
    "\n",
    "for label in labels:\n",
    "    face_path = f\"{directory}/{label}/video/head3/004\"\n",
    "    img = Image.open(face_path)\n",
    "    img_cropped = mtcnn(img)\n",
    "\n",
    "    try:\n",
    "        img_embedding = resnet(img_cropped.unsqueeze(0))\n",
    "    except Exception:\n",
    "        print(img_cropped, label, img)\n",
    "        plt.imshow(img)\n",
    "        break\n",
    "\n",
    "    embeddings_face[label] = img_embedding\n",
    "\n",
    "    try:\n",
    "        audio_path = f\"{directory}/{label}/audio/sa1.wav\"\n",
    "        embedding = inference(audio_path).reshape(1,512)\n",
    "        embeddings_audio[label] = embedding\n",
    "    except Exception:\n",
    "        print(label, \" Error in audio embedding extraction\")\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing Face and Audio embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frontal pose : \n",
    "\n",
    "Intial accuracy\n",
    "1. Face Accuracy 100%\n",
    "2. Audio recognition accuracy 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Accuracy is : 100.0 %\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "correct_detection_count = 0\n",
    "for label in labels:\n",
    "    \n",
    "    # extract embedding \n",
    "    audio_emd = inference(f\"{directory}/{label}/audio/sa2.wav\").reshape(1,512)\n",
    "    img_path = f\"{directory}/{label}/video/head2/001\"\n",
    "    img = Image.open(img_path)\n",
    "    img_cropped = mtcnn(img)\n",
    "    img_embedding = resnet(img_cropped.unsqueeze(0))\n",
    "    \n",
    "    label_idx = []\n",
    "    audio_sim = []\n",
    "    face_sim = []\n",
    "    for test_label in labels:\n",
    "        # Audio similarity\n",
    "        distance = cdist(audio_emd, embeddings_audio[test_label], metric=\"cosine\")\n",
    "        audio_sim.append(1- distance[0])\n",
    "        # Face similarity\n",
    "        cos_similarity = F.cosine_similarity(img_embedding, embeddings_face[test_label])\n",
    "        face_sim.append(float(cos_similarity))\n",
    "    \n",
    "    audio_sim = np.array(audio_sim).flatten()/max(audio_sim)\n",
    "    face_sim = np.array(face_sim).flatten()/max(face_sim)\n",
    "    \n",
    "    similarity = face_sim*0.5 + audio_sim*0.5\n",
    "    match_label = labels[np.argmax(similarity)]\n",
    "        \n",
    "    if match_label == label:\n",
    "        correct_detection_count += 1\n",
    "accuracy = correct_detection_count/ 43\n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)\n",
    "print(\"Accuracy is :\" , accuracy*100 , \"%\" )  \n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side pose : \n",
    "\n",
    "Intial accuracy\n",
    "1. Face Accuracy 95.6%\n",
    "2. Audio recognition accuracy 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Accuracy is : 100.0 %\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "correct_detection_count = 0\n",
    "for label in labels:\n",
    "    \n",
    "    # extract embedding \n",
    "    audio_emd = inference(f\"{directory}/{label}/audio/sa2.wav\").reshape(1,512)\n",
    "    img_path = f\"{directory}/{label}/video/head2/025\"\n",
    "    img = Image.open(img_path)\n",
    "    img_cropped = mtcnn(img)\n",
    "    img_embedding = resnet(img_cropped.unsqueeze(0))\n",
    "    \n",
    "    label_idx = []\n",
    "    audio_sim = []\n",
    "    face_sim = []\n",
    "    for test_label in labels:\n",
    "        # Audio similarity\n",
    "        distance = cdist(audio_emd, embeddings_audio[test_label], metric=\"cosine\")\n",
    "        audio_sim.append(1- distance[0])\n",
    "        # Face similarity\n",
    "        cos_similarity = F.cosine_similarity(img_embedding, embeddings_face[test_label])\n",
    "        face_sim.append(float(cos_similarity))\n",
    "    \n",
    "    audio_sim = np.array(audio_sim).flatten()/max(audio_sim)\n",
    "    face_sim = np.array(face_sim).flatten()/max(face_sim)\n",
    "    \n",
    "    similarity = face_sim*0.5 + audio_sim*0.5\n",
    "    match_label = labels[np.argmax(similarity)]\n",
    "        \n",
    "    if match_label == label:\n",
    "        correct_detection_count += 1\n",
    "accuracy = correct_detection_count/ 43\n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)\n",
    "print(\"Accuracy is :\" , accuracy*100 , \"%\" )  \n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side Pose with Noisy Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Accuracy is : 100.0 %\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "correct_detection_count = 0\n",
    "directory = \"VIDTIMIT\"\n",
    "for label in labels:\n",
    "    \n",
    "    # extract embedding \n",
    "    dir = f\"{directory}/{label}/audio/\"\n",
    "    audio_list = os.listdir(dir)\n",
    "    file_list = [file for file in audio_list if os.path.isfile(os.path.join(dir, file))]\n",
    "    audio_wav = f\"{dir}/{random.choice(file_list)}\"\n",
    "    audio_emd = inference(audio_wav).reshape(1,512)\n",
    "    \n",
    "    img_path = f\"{directory}/{label}/video/head2/025\"\n",
    "    img = Image.open(img_path)\n",
    "    img_cropped = mtcnn(img)\n",
    "    img_embedding = resnet(img_cropped.unsqueeze(0))\n",
    "    \n",
    "    label_idx = []\n",
    "    audio_sim = []\n",
    "    face_sim = []\n",
    "    for test_label in labels:\n",
    "        # Audio similarity\n",
    "        distance = cdist(audio_emd, embeddings_audio[test_label], metric=\"cosine\")\n",
    "        audio_sim.append(1- distance[0])\n",
    "        # Face similarity\n",
    "        cos_similarity = F.cosine_similarity(img_embedding, embeddings_face[test_label])\n",
    "        face_sim.append(float(cos_similarity))\n",
    "    \n",
    "    audio_sim = np.array(audio_sim).flatten()/max(audio_sim)\n",
    "    face_sim = np.array(face_sim).flatten()/max(face_sim)\n",
    "    \n",
    "    similarity = face_sim*0.5 + audio_sim*0.5\n",
    "    match_label = labels[np.argmax(similarity)]\n",
    "        \n",
    "    if match_label == label:\n",
    "        correct_detection_count += 1\n",
    "accuracy = correct_detection_count/ 43\n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)\n",
    "print(\"Accuracy is :\" , accuracy*100 , \"%\" )  \n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "1. As can be seen, Even with noisy audio and side face, accuracy is 100% when audio and video both methods are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
