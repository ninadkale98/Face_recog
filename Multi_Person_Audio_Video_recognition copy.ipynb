{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio import Inference\n",
    "from scipy.spatial.distance import cdist\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from moviepy.editor import VideoFileClip\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../.cache/torch/pyannote/models--pyannote--embedding/snapshots/20b2db779562a3141f5eadd34a0232dbcd56d620/pytorch_model.bin`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../.cache/torch/pyannote/models--pyannote--embedding/snapshots/20b2db779562a3141f5eadd34a0232dbcd56d620/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "mtcnn = MTCNN()\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "model = Model.from_pretrained(\"pyannote/embedding\", use_auth_token=\"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\")\n",
    "inference = Inference(model, window=\"whole\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person 1 Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"video\"\n",
    "path = f\"{directory}/N.JPG\"\n",
    "img = Image.open(path)\n",
    "img = img.rotate(-90)\n",
    "img_cropped = mtcnn(img)\n",
    "person1_embedding = resnet(img_cropped.unsqueeze(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person 2 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"video\"\n",
    "path = f\"{directory}/K.JPG\"\n",
    "img = Image.open(path)\n",
    "img = img.rotate(-90)\n",
    "img_cropped = mtcnn(img)\n",
    "person2_embedding = resnet(img_cropped.unsqueeze(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract audio from mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in video/common_aud.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "video = VideoFileClip(f'{directory}/Ninad_with_vaishnavi_voice.mp4')\n",
    "audio = video.audio\n",
    "audio.write_audiofile(f'{directory}/common_aud.wav')\n",
    "video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 17.7M/17.7M [00:00<00:00, 49.4MB/s]\n",
      "Downloading (…)/2022.07/config.yaml: 100%|██████████| 318/318 [00:00<00:00, 2.98MB/s]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)ain/hyperparams.yaml: 100%|██████████| 1.92k/1.92k [00:00<00:00, 17.2MB/s]\n",
      "Downloading embedding_model.ckpt: 100%|██████████| 83.3M/83.3M [00:01<00:00, 62.1MB/s]\n",
      "Downloading (…)an_var_norm_emb.ckpt: 100%|██████████| 1.92k/1.92k [00:00<00:00, 19.4MB/s]\n",
      "Downloading classifier.ckpt: 100%|██████████| 5.53M/5.53M [00:00<00:00, 25.1MB/s]\n",
      "Downloading (…)in/label_encoder.txt: 100%|██████████| 129k/129k [00:00<00:00, 6.30MB/s]\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", use_auth_token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<pyannote.core.annotation.Annotation at 0x7f3c7eddc510>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file = \"video/common_aud.wav\"\n",
    "\n",
    "diarization = pipeline(audio_file)\n",
    "diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"audio.rttm\", \"w\") as rttm:\n",
    "    diarization.write_rttm(rttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m segment \u001b[39min\u001b[39;00m diarization[\u001b[39m'\u001b[39;49m\u001b[39mpyAudioAnalysis\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mspeech_turns\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m      2\u001b[0m     speaker_id \u001b[39m=\u001b[39m segment[\u001b[39m'\u001b[39m\u001b[39mtrack\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m     start_time \u001b[39m=\u001b[39m segment[\u001b[39m'\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/bio1/lib/python3.11/site-packages/pyannote/core/annotation.py:848\u001b[0m, in \u001b[0;36mAnnotation.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Segment):\n\u001b[1;32m    846\u001b[0m     key \u001b[39m=\u001b[39m (key, \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 848\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracks[key[\u001b[39m0\u001b[39;49m]][key[\u001b[39m1\u001b[39m]]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'p'"
     ]
    }
   ],
   "source": [
    "for segment in diarization['pyAudioAnalysis']['speech_turns']:\n",
    "    speaker_id = segment['track']\n",
    "    start_time = segment['start']\n",
    "    end_time = segment['end']\n",
    "    \n",
    "    # Convert time values from seconds to milliseconds\n",
    "    start_time_ms = int(start_time * 1000)\n",
    "    end_time_ms = int(end_time * 1000)\n",
    "    \n",
    "    # Load the original audio file\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    \n",
    "    # Extract the speaker's audio segment\n",
    "    speaker_segment = audio[start_time_ms:end_time_ms]\n",
    "    \n",
    "    # Save the speaker's audio segment to a file\n",
    "    output_file = f\"speaker_{speaker_id}.wav\"\n",
    "    speaker_segment.export(output_file, format=\"wav\")\n",
    "    print(f\"Saved audio segment for speaker {speaker_id} to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyAudioAnalysis import audioSegmentation as aS\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, _ , _ = aS.speaker_diarization(audio_file, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m segment \u001b[39min\u001b[39;00m result:\n\u001b[0;32m----> 2\u001b[0m     speaker_id \u001b[39m=\u001b[39m segment[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m      3\u001b[0m     start_time \u001b[39m=\u001b[39m segment[\u001b[39m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m     end_time \u001b[39m=\u001b[39m segment[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "for segment in result:\n",
    "    speaker_id = segment[2]\n",
    "    start_time = segment[0]\n",
    "    end_time = segment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m save_diarized_segments(audio_file, diarization_labels, output_directory)\n",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m, in \u001b[0;36msave_diarized_segments\u001b[0;34m(audio_file, diarization_labels, output_directory)\u001b[0m\n\u001b[1;32m      2\u001b[0m audio \u001b[39m=\u001b[39m AudioSegment\u001b[39m.\u001b[39mfrom_file(audio_file)\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m diarization_labels:\n\u001b[0;32m----> 5\u001b[0m     label_start \u001b[39m=\u001b[39m label[\u001b[39m0\u001b[39;49m] \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m  \u001b[39m# Convert start time to milliseconds\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     label_end \u001b[39m=\u001b[39m label[\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m  \u001b[39m# Convert end time to milliseconds\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     segment \u001b[39m=\u001b[39m audio[label_start:label_end]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "save_diarized_segments(audio_file, diarization_labels, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../.cache/torch/pyannote/models--pyannote--embedding/snapshots/20b2db779562a3141f5eadd34a0232dbcd56d620/pytorch_model.bin`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../.cache/torch/pyannote/models--pyannote--embedding/snapshots/20b2db779562a3141f5eadd34a0232dbcd56d620/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "model = Model.from_pretrained(\"pyannote/embedding\", use_auth_token=\"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\")\n",
    "inference = Inference(model, window=\"whole\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = f\"{directory}/N_audio.wav\"\n",
    "Person1_audio_embd = inference(audio_path).reshape(1,512)\n",
    "\n",
    "audio_path = f\"{directory}/N_Test_audio.wav\"\n",
    "Person1_audio_Test_embd = inference(audio_path).reshape(1,512)\n",
    "\n",
    "audio_path = f\"{directory}/K_audio.wav\"\n",
    "Person2_audio_embd = inference(audio_path).reshape(1,512)\n",
    "\n",
    "audio_path = f\"{directory}/K_Test_audio.wav\"\n",
    "Person2_audio_Test_embd = inference(audio_path).reshape(1,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Similarity between image of same person is :  0.7992940634399912\n",
      "Similarity between image of diff person is :  0.18052444833048087\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"-\"*50)\n",
    "cos_similarity = 1 - cdist(Person1_audio_embd, Person1_audio_Test_embd, metric=\"cosine\")\n",
    "print(\"Similarity between image of same person is : \", float(cos_similarity))\n",
    "cos_similarity = 1 - cdist(Person2_audio_embd, Person1_audio_Test_embd, metric=\"cosine\")\n",
    "print(\"Similarity between image of diff person is : \", float(cos_similarity))\n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Mp4 recorded video with two person\n",
    "\n",
    "1. goal is to find out who is speaking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in video/output.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "[[1.]] [[0.18131606]]\n",
      "Audio ->  P1H [[1.]] [[0.18131606]]\n",
      "--------------------------------------------------\n",
      "Similarity of cuurent face : 0.7657683491706848 0.19879299402236938\n",
      "Similarity of cuurent face : 0.3451361060142517 0.849254310131073\n",
      "--------------------------------------------------\n",
      "Detected person is :  Person1 Sure\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "video_name = \"Ninad_1.mp4\"\n",
    "mp4_file = f'{directory}/{video_name}'\n",
    "\n",
    "# Extract audio\n",
    "video = VideoFileClip(mp4_file)\n",
    "audio = video.audio\n",
    "audio.write_audiofile(f'{directory}/output.wav')\n",
    "video.close()\n",
    "\n",
    "# Extract Audio Embeddings\n",
    "audio_path = f\"{directory}/output.wav\"\n",
    "Audio_Embed = inference(audio_path).reshape(1,512)\n",
    "\n",
    "\n",
    "video = cv2.VideoCapture(f'video/{video_name}')\n",
    "for _ in range(10):\n",
    "    _, frame = video.read()\n",
    "_, frame = video.read()\n",
    "#img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "video.release()\n",
    "cv2.imwrite(\"video/output.jpg\", frame)\n",
    "\n",
    "\n",
    "img = Image.open(\"video/output.jpg\")\n",
    "faces = mtcnn.detect(img)\n",
    "\n",
    "face_detect_embd = []\n",
    "bbox = []\n",
    "for BoundingBox in faces[0]:\n",
    "    img_cropped = img.crop(BoundingBox)\n",
    "    img_cropped = mtcnn(img_cropped)\n",
    "    face_detect_embd.append( resnet(img_cropped.unsqueeze(0)))\n",
    "    bbox.append(BoundingBox)\n",
    "\n",
    "person1_audio_similarity = 1 -  cdist(Audio_Embed, Person1_audio_embd , metric=\"cosine\")\n",
    "person2_audio_similarity = 1 -  cdist(Audio_Embed, Person2_audio_embd , metric=\"cosine\")\n",
    "\n",
    "\n",
    "if person1_audio_similarity > person2_audio_similarity and person1_audio_similarity > 0.75:\n",
    "    audio_det = \"P1H\"\n",
    "elif person1_audio_similarity > person2_audio_similarity and person1_audio_similarity < 0.75 and person1_audio_similarity > 0.6  :\n",
    "    audio_det = \"P1L\"\n",
    "elif person2_audio_similarity > person1_audio_similarity and person2_audio_similarity > 0.75:\n",
    "    audio_det = \"P2H\"\n",
    "elif person2_audio_similarity > person1_audio_similarity and person2_audio_similarity < 0.75 and person2_audio_similarity > 0.6  :\n",
    "    audio_det = \"P2L\"\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)\n",
    "print(person1_audio_similarity , person2_audio_similarity)\n",
    "print(\"Audio -> \", audio_det , person1_audio_similarity , person2_audio_similarity)\n",
    "print(\"-\"*50)\n",
    "detected_person = None\n",
    "\n",
    "for embd in face_detect_embd:\n",
    "    person1_face_similarity = F.cosine_similarity(person1_embedding, embd)\n",
    "    person2_face_similarity = F.cosine_similarity(person2_embedding, embd)\n",
    "    print(\"Similarity of cuurent face :\" , float(person1_face_similarity), float(person2_face_similarity))\n",
    "    if person1_face_similarity > person2_face_similarity:\n",
    "        if person1_face_similarity > 0.75 and audio_det == \"P1L\" or audio_det == \"P1H\"  : \n",
    "            detected_person = \"Person1 Sure\"\n",
    "        elif person1_face_similarity > 0.6 and audio_det == \"P1H\": \n",
    "            detected_person = \"Person1 Maybe\"\n",
    "    \n",
    "    elif person2_face_similarity > person1_face_similarity:\n",
    "        if person2_face_similarity > 0.75 and (audio_det == \"P2L\" or audio_det == \"P2H\") : \n",
    "            detected_person = \"Person2 Sure\"\n",
    "        elif person2_face_similarity > 0.6 and audio_det == \"P2H\": \n",
    "            detected_person = \"Person2 Maybe\"\n",
    "print(\"-\"*50)            \n",
    "print(\"Detected person is : \" , detected_person)\n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in video/output.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Input file : Ninad_1.mp4\n",
      "--------------------------------------------------\n",
      "Person1 : Ninad | Person2 : Kartikeya\n",
      "--------------------------------------------------\n",
      "Detected person is : Person 1  | Prob :  [0.68053542 0.31946458]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "video_name = \"Ninad_1.mp4\"\n",
    "#video_name = \"Ninad_side.mp4\"\n",
    "#video_name = \"Kartikeya_1.mp4\"\n",
    "#video_name = \"Kartikeya_occ.mp4\"\n",
    "#video_name = \"Ninad_occ.mp4\"\n",
    "mp4_file = f'{directory}/{video_name}'\n",
    "\n",
    "# Extract audio\n",
    "video = VideoFileClip(mp4_file)\n",
    "audio = video.audio\n",
    "audio.write_audiofile(f'{directory}/output.wav')\n",
    "video.close()\n",
    "\n",
    "# Extract Audio Embeddings\n",
    "audio_path = f\"{directory}/output.wav\"\n",
    "Audio_Embed = inference(audio_path).reshape(1,512)\n",
    "\n",
    "\n",
    "video = cv2.VideoCapture(f'video/{video_name}')\n",
    "for _ in range(10):\n",
    "    _, frame = video.read()\n",
    "_, frame = video.read()\n",
    "#img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "video.release()\n",
    "cv2.imwrite(\"video/output.jpg\", frame)\n",
    "\n",
    "\n",
    "img = Image.open(\"video/output.jpg\")\n",
    "faces = mtcnn.detect(img)\n",
    "\n",
    "face_detect_embd = []\n",
    "bbox = []\n",
    "for BoundingBox in faces[0]:\n",
    "    img_cropped = img.crop(BoundingBox)\n",
    "    img_cropped = mtcnn(img_cropped)\n",
    "    face_detect_embd.append( resnet(img_cropped.unsqueeze(0)))\n",
    "    bbox.append(BoundingBox)\n",
    "\n",
    "person1_audio_similarity = 1 -  cdist(Audio_Embed, Person1_audio_embd , metric=\"cosine\")\n",
    "person2_audio_similarity = 1 -  cdist(Audio_Embed, Person2_audio_embd , metric=\"cosine\")\n",
    "\n",
    "Audio_prob = [person1_audio_similarity, person2_audio_similarity]\n",
    "Audio_prob = np.array(Audio_prob).flatten()/sum(Audio_prob)[0]\n",
    "#Audio_prob = Audio_prob/np.sum(Audio_prob)\n",
    "\n",
    "detected_person = None\n",
    "Face_prob = []\n",
    "\n",
    "for embd in face_detect_embd:\n",
    "    person1_face_similarity = F.cosine_similarity(person1_embedding, embd)\n",
    "    person2_face_similarity = F.cosine_similarity(person2_embedding, embd)\n",
    "    Face_prob.append([float(person1_face_similarity),float( person2_face_similarity)])\n",
    "\n",
    "Face_prob = np.array(Face_prob)\n",
    "Face_prob = np.sum(Face_prob, axis=0)\n",
    "Face_prob = Face_prob/np.sum(Face_prob)\n",
    "\n",
    "prob = 0.5*Face_prob + 0.5*Audio_prob\n",
    "print(\"-\"*50)  \n",
    "print(\"Input file :\" , video_name)\n",
    "print(\"-\"*50)\n",
    "print(\"Person1 : Ninad | Person2 : Kartikeya\")\n",
    "print(\"-\"*50)            \n",
    "print(\"Detected person is : Person\" , np.argmax(prob) + 1 , \" | Prob : \",prob )\n",
    "print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Person2'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_person\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"video\"\n",
    "path = f\"{directory}/N.JPG\"\n",
    "img = Image.open(path)\n",
    "img = img.rotate(-90)\n",
    "img_cropped = mtcnn(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2695, -0.2773, -0.3008,  ..., -0.1680, -0.2070, -0.1992],\n",
       "         [-0.2852, -0.2852, -0.3086,  ..., -0.2070, -0.2148, -0.2070],\n",
       "         [-0.2852, -0.2930, -0.3008,  ..., -0.1992, -0.1992, -0.1992],\n",
       "         ...,\n",
       "         [ 0.1836,  0.1758,  0.1445,  ..., -0.2695, -0.4180, -0.4180],\n",
       "         [ 0.1523,  0.1680,  0.1523,  ...,  0.1445, -0.2227, -0.3789],\n",
       "         [ 0.1602,  0.1992,  0.1836,  ...,  0.4102,  0.1758, -0.1445]],\n",
       "\n",
       "        [[-0.3242, -0.3320, -0.3398,  ..., -0.1992, -0.2383, -0.2383],\n",
       "         [-0.3398, -0.3398, -0.3555,  ..., -0.2383, -0.2461, -0.2383],\n",
       "         [-0.3320, -0.3477, -0.3398,  ..., -0.2305, -0.2305, -0.2305],\n",
       "         ...,\n",
       "         [ 0.0820,  0.0820,  0.0586,  ..., -0.4102, -0.4961, -0.4883],\n",
       "         [ 0.0586,  0.0742,  0.0586,  ..., -0.1133, -0.3789, -0.4727],\n",
       "         [ 0.0742,  0.1055,  0.0898,  ...,  0.0898, -0.0977, -0.3164]],\n",
       "\n",
       "        [[-0.3789, -0.3711, -0.3789,  ..., -0.2227, -0.2461, -0.2383],\n",
       "         [-0.3867, -0.3711, -0.3789,  ..., -0.2539, -0.2539, -0.2461],\n",
       "         [-0.3789, -0.3867, -0.3789,  ..., -0.2383, -0.2383, -0.2383],\n",
       "         ...,\n",
       "         [-0.0430, -0.0430, -0.0664,  ..., -0.4883, -0.5273, -0.5039],\n",
       "         [-0.0664, -0.0508, -0.0586,  ..., -0.2852, -0.4727, -0.5117],\n",
       "         [-0.0586, -0.0195, -0.0195,  ..., -0.1289, -0.2695, -0.4258]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
