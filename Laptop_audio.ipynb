{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/easgrad/ninadnar/miniconda3/envs/bio1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio import Inference\n",
    "from scipy.spatial.distance import cdist\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from moviepy.editor import VideoFileClip\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../.cache/torch/pyannote/models--pyannote--embedding/snapshots/20b2db779562a3141f5eadd34a0232dbcd56d620/pytorch_model.bin`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../.cache/torch/pyannote/models--pyannote--embedding/snapshots/20b2db779562a3141f5eadd34a0232dbcd56d620/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "mtcnn = MTCNN()\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "model = Model.from_pretrained(\"pyannote/embedding\", use_auth_token=\"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\")\n",
    "inference = Inference(model, window=\"whole\")\n",
    "directory = \"video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in video/AT.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in video/A.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in video/HT.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in video/H.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "video = VideoFileClip(f'{directory}/anshul_test.mov')\n",
    "audio = video.audio\n",
    "audio.write_audiofile(f'{directory}/AT.wav')\n",
    "video.close()\n",
    "\n",
    "video = VideoFileClip(f'{directory}/anshul.mov')\n",
    "audio = video.audio\n",
    "audio.write_audiofile(f'{directory}/A.wav')\n",
    "video.close()\n",
    "\n",
    "video = VideoFileClip(f'{directory}/harish_test.mov')\n",
    "audio = video.audio\n",
    "audio.write_audiofile(f'{directory}/HT.wav')\n",
    "video.close()\n",
    "\n",
    "video = VideoFileClip(f'{directory}/Harish.mov')\n",
    "audio = video.audio\n",
    "audio.write_audiofile(f'{directory}/H.wav')\n",
    "video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../.cache/torch/pyannote/models--pyannote--embedding/snapshots/20b2db779562a3141f5eadd34a0232dbcd56d620/pytorch_model.bin`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../.cache/torch/pyannote/models--pyannote--embedding/snapshots/20b2db779562a3141f5eadd34a0232dbcd56d620/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "model = Model.from_pretrained(\"pyannote/embedding\", use_auth_token=\"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\")\n",
    "inference = Inference(model, window=\"whole\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = f\"{directory}/A.wav\"\n",
    "Person1_audio_embd = inference(audio_path).reshape(1,512)\n",
    "\n",
    "audio_path = f\"{directory}/AT.wav\"\n",
    "Person1_audio_Test_embd = inference(audio_path).reshape(1,512)\n",
    "\n",
    "audio_path = f\"{directory}/H.wav\"\n",
    "Person2_audio_embd = inference(audio_path).reshape(1,512)\n",
    "\n",
    "audio_path = f\"{directory}/HT.wav\"\n",
    "Person2_audio_Test_embd = inference(audio_path).reshape(1,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Similarity between audio of same person is :  0.3271322160895027\n",
      "Similarity between audio of diff person is :  0.23514698298464565\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"-\"*50)\n",
    "cos_similarity = 1 - cdist(Person1_audio_embd, Person1_audio_Test_embd, metric=\"cosine\")\n",
    "print(\"Similarity between audio of same person is : \", float(cos_similarity))\n",
    "cos_similarity = 1 - cdist(Person2_audio_embd, Person1_audio_Test_embd, metric=\"cosine\")\n",
    "print(\"Similarity between audio of diff person is : \", float(cos_similarity))\n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
